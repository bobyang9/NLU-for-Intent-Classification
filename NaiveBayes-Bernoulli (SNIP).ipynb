{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "# from load_data import *\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnipsDataLoader():\n",
    "    def __init__(self, train_path, valid_path=None, test_path=None):\n",
    "        self.train_data = self.load_dataset(train_path)\n",
    "        self.valid_data = self.load_dataset(valid_path) if valid_path != None else None\n",
    "        self.test_data = self.load_dataset(test_path) if test_path != None else None\n",
    "        \n",
    "        self.create_label_mapping()\n",
    "        \n",
    "    def load_dataset(self, data_path):\n",
    "        data = pd.read_csv(data_path, header=None, delimiter='\\t')\n",
    "        data.columns = ['labels', 'texts']\n",
    "        output = {'X': data['texts'], 'y': data['labels']}\n",
    "        return output\n",
    "        \n",
    "    def create_label_mapping(self):\n",
    "        self.text_to_index_label_mapping = {}\n",
    "        self.index_to_text_label_mapping = {}\n",
    "        for i, label in enumerate(self.train_data['y'].unique()):\n",
    "            self.text_to_index_label_mapping[label] = i\n",
    "            self.index_to_text_label_mapping[i] = label\n",
    "        \n",
    "        self.train_data['y'] = \\\n",
    "            self.train_data['y'].map(lambda x: self.text_to_index_label_mapping[x])\n",
    "        if self.valid_data:\n",
    "            self.valid_data['y'] = \\\n",
    "                self.valid_data['y'].map(lambda x: self.text_to_index_label_mapping[x])\n",
    "        if self.test_data:\n",
    "            self.test_data['y'] = \\\n",
    "                self.test_data['y'].map(lambda x: self.text_to_index_label_mapping[x])\n",
    "    \n",
    "    def split_train_valid(self, valid_size, keep_class_ratios=True, random_state=0):\n",
    "        X, y = self.train_data['X'], self.train_data['y']\n",
    "        if keep_class_ratios:\n",
    "            X_train, X_valid, y_train, y_valid = \\\n",
    "                train_test_split(X, y, test_size=valid_size, random_state=random_state, stratify=y)\n",
    "        else:\n",
    "            X_train, X_valid, y_train, y_valid = \\\n",
    "                train_test_split(X, y, test_size=valid_size, random_state=random_state)\n",
    "            \n",
    "        self.train_data = {'X': X_train, 'y': y_train}\n",
    "        self.valid_data = {'X': X_valid, 'y': y_valid}\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return list(self.train_data['X']), self.train_data['y'].to_numpy()\n",
    "    \n",
    "    def get_valid_data(self):\n",
    "        return list(self.valid_data['X']), self.valid_data['y'].to_numpy()\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return list(self.test_data['X']), self.test_data['y'].to_numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "class FeatureExtractor():\n",
    "    def __init__(self, X_train, X_valid=None, X_test=None):\n",
    "        self.X_train = X_train\n",
    "        self.X_valid = X_valid\n",
    "        self.X_test = X_test\n",
    "    \n",
    "    def extract_features(self, keep_words_threshold=5):\n",
    "        self.keep_words_threshold = keep_words_threshold\n",
    "        \n",
    "        self.X_train = self.preprocess_data(self.X_train)\n",
    "        if self.X_valid:\n",
    "            self.X_valid = self.preprocess_data(self.X_valid)\n",
    "        if self.X_test:\n",
    "            self.X_test = self.preprocess_data(self.X_test)\n",
    "        \n",
    "        self.create_vocab(self.X_train)\n",
    "        \n",
    "        self.X_train = self.create_encodings(self.X_train)\n",
    "        if self.X_valid:\n",
    "            self.X_valid = self.create_encodings(self.X_valid)\n",
    "        if self.X_test:\n",
    "            self.X_test = self.create_encodings(self.X_test)\n",
    "    \n",
    "    def preprocess_data(self, text_data):\n",
    "        output = []\n",
    "        for example in text_data:\n",
    "            words = [word.lower() for word in example.split()]\n",
    "            output.append(words)\n",
    "        return output\n",
    "    \n",
    "    def create_vocab(self, text_data):\n",
    "        word_occurences = collections.defaultdict(int)\n",
    "        for example in text_data:\n",
    "            word_counts = self.get_word_counts(example)\n",
    "            for word in word_counts.keys():\n",
    "                word_occurences[word] += 1\n",
    "        \n",
    "        vocab_words = [word for word in sorted(word_occurences.keys()) \n",
    "                       if word_occurences[word] >= self.keep_words_threshold]\n",
    "        self.vocab = {word: index for index, word in enumerate(vocab_words)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_encodings(self, text_data):\n",
    "        num_examples = len(text_data)\n",
    "        encodings = np.zeros((num_examples, self.vocab_size))\n",
    "        \n",
    "        for row, example in enumerate(text_data):\n",
    "            word_counts = self.get_word_counts(example)\n",
    "            for word, count in word_counts.items():\n",
    "                if word in self.vocab:\n",
    "                    col = self.vocab[word]\n",
    "                    encodings[row, col] = 1\n",
    "                    \n",
    "        return encodings\n",
    "                    \n",
    "    def get_word_counts(self, word_list):\n",
    "        counts = collections.defaultdict(int)\n",
    "        for word in word_list:\n",
    "            counts[word] += 1\n",
    "        return counts\n",
    "    \n",
    "    def get_train_encodings(self):\n",
    "        return self.X_train\n",
    "    \n",
    "    def get_valid_encodings(self):\n",
    "        return self.X_valid\n",
    "    \n",
    "    def get_test_encodings(self):\n",
    "        return self.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './snips/snips_train_actual.csv'\n",
    "TEST_PATH = './snips/snips_test_actual.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SnipsDataLoader(TRAIN_PATH, None, TEST_PATH)\n",
    "data_loader.split_train_valid(valid_size=0.05, keep_class_ratios=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_loader.get_train_data()\n",
    "X_valid, y_valid = data_loader.get_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(X_train, X_valid)\n",
    "feature_extractor.extract_features(keep_words_threshold=5)\n",
    "X_train = feature_extractor.get_train_encodings()\n",
    "X_valid = feature_extractor.get_valid_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_examples, vocab_size = X.shape\n",
    "        num_labels = np.amax(y) + 1\n",
    "        y_one_hot = np.eye(num_labels)[y]\n",
    "        \n",
    "        self.vocab_probs_1 = (1 + np.dot(X.T, y_one_hot)) / (2 + np.sum(y_one_hot, axis=0))\n",
    "        self.vocab_probs_0 = 1 - self.vocab_probs_1\n",
    "        self.prior_probs = np.mean(y_one_hot, axis=0)\n",
    "        self.vocab_log_probs_1 = np.log(self.vocab_probs_1)\n",
    "        self.vocab_log_probs_0 = np.log(self.vocab_probs_0)\n",
    "        self.prior_log_probs = np.log(self.prior_probs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        post_probs = np.dot(X, self.vocab_log_probs_1) + np.dot(1 - X, self.vocab_log_probs_0) + self.prior_log_probs\n",
    "        predictions = np.argmax(post_probs, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, targets):\n",
    "    return np.mean(predictions == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782608695652174"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BernoulliNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_valid)\n",
    "calculate_accuracy(y_predict, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        97\n",
      "           1       0.99      1.00      0.99        99\n",
      "           2       1.00      1.00      1.00       100\n",
      "           3       1.00      1.00      1.00        98\n",
      "           4       0.92      0.94      0.93        98\n",
      "           5       0.99      0.93      0.96        98\n",
      "           6       0.96      0.98      0.97       100\n",
      "\n",
      "    accuracy                           0.98       690\n",
      "   macro avg       0.98      0.98      0.98       690\n",
      "weighted avg       0.98      0.98      0.98       690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'AddToPlaylist',\n",
       " 1: 'BookRestaurant',\n",
       " 2: 'GetWeather',\n",
       " 3: 'RateBook',\n",
       " 4: 'SearchCreativeWork',\n",
       " 5: 'SearchScreeningEvent',\n",
       " 6: 'PlayMusic'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.index_to_text_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
