{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from load_data import *\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './snips/snips_train_actual.csv'\n",
    "TEST_PATH = './snips/snips_test_actual.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SnipsDataLoader(TRAIN_PATH, None, TEST_PATH)\n",
    "data_loader.split_train_valid(valid_size=0.05, keep_class_ratios=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_loader.get_train_data()\n",
    "X_valid, y_valid = data_loader.get_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(X_train, X_valid)\n",
    "feature_extractor.extract_features(keep_words_threshold=5)\n",
    "X_train = feature_extractor.get_train_encodings()\n",
    "X_valid = feature_extractor.get_valid_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression():\n",
    "    def __init__(self, training_rate, num_iter):\n",
    "        self.training_rate = training_rate\n",
    "        self.num_iter = num_iter\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "    fit(self, X, y): fits the weights and biases based on X (training set design matrix) and y (training set ground truths).\n",
    "    Input: X -- (num_examples, vocab_size) shape.\n",
    "    Input: y -- (num_examples, ) shape.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        num_examples, vocab_size = X.shape\n",
    "        num_labels = np.amax(y) + 1\n",
    "        y_one_hot = np.eye(num_labels)[y].T\n",
    "        y_val_one_hot = np.eye(num_labels)[y_val].T\n",
    "        \n",
    "        self.weights = np.random.normal(loc=0.0, scale=(1/np.sqrt(X.shape[0])), size=(num_labels, vocab_size))\n",
    "        self.biases = np.zeros((num_labels,1))\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            predictions = self.predict(X)\n",
    "            self.weights = self.weights - self.training_rate * np.dot(predictions - y_one_hot, X)\n",
    "            self.biases = self.biases - self.training_rate * np.reshape(np.sum(predictions - y_one_hot, axis=1), (-1, 1))\n",
    "            predictions_train = self.predict(X)\n",
    "            predictions_val = self.predict(X_val)\n",
    "            train_loss = self.get_loss(predictions_train, y_one_hot)\n",
    "            val_loss = self.get_loss(predictions_val, y_val_one_hot)\n",
    "            train_accuracy = self.find_accuracy(predictions_train, y)\n",
    "            val_accuracy = self.find_accuracy(predictions_val, y_val)\n",
    "            if(i%5 == 0):\n",
    "                print(\"Iteration %d: train_loss: %f, val_loss: %f, train_accuracy: %f, val_accuracy: %f\"%(i, train_loss, val_loss, train_accuracy, val_accuracy))   \n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    predict(self, X): Returns predictions of the labels given the design matrix X.\n",
    "    Input: X -- (num_examples, vocab_size) shape.\n",
    "    Output: predictions -- (num_labels, num_examples) shape.\n",
    "    \"\"\"\n",
    "    def predict(self, X):\n",
    "        return scipy.special.softmax(np.dot(self.weights, X.T) + self.biases, axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    get_loss(self, X, y): Returns the loss.\n",
    "    Input: predictions -- (num_examples, num_labels) shape.\n",
    "    Input: y -- (num_examples, num_labels) shape. (has to be one-hot)\n",
    "    Output: loss -- scalar.\n",
    "    \"\"\"\n",
    "    def get_loss(self, predictions, y):\n",
    "        return -np.average(np.sum(np.multiply(y, np.log(predictions)), axis=0))\n",
    "    \n",
    "    \"\"\"\n",
    "    find_accuracy(self, predictions, y): returns the accuracy\n",
    "    Input: predictions -- (num_examples, num_labels) shape.\n",
    "    Input: y -- (num_examples, num_labels) shape. (has to be one-hot)\n",
    "    Output: loss -- scalar.\n",
    "    \"\"\"\n",
    "    def find_accuracy(self, predictions, y):\n",
    "        predictions = np.argmax(predictions, axis=0)\n",
    "        return np.average(predictions==y)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: train_loss: 0.656036, val_loss: 0.656886, train_accuracy: 0.855888, val_accuracy: 0.840580\n",
      "Iteration 5: train_loss: 0.329867, val_loss: 0.341177, train_accuracy: 0.905606, val_accuracy: 0.905797\n",
      "Iteration 10: train_loss: 0.181788, val_loss: 0.184558, train_accuracy: 0.960669, val_accuracy: 0.962319\n",
      "Iteration 15: train_loss: 0.154401, val_loss: 0.157129, train_accuracy: 0.965709, val_accuracy: 0.966667\n",
      "Iteration 20: train_loss: 0.137029, val_loss: 0.140026, train_accuracy: 0.968535, val_accuracy: 0.971014\n",
      "Iteration 25: train_loss: 0.124700, val_loss: 0.128031, train_accuracy: 0.971055, val_accuracy: 0.972464\n",
      "Iteration 30: train_loss: 0.115364, val_loss: 0.119045, train_accuracy: 0.973347, val_accuracy: 0.973913\n",
      "Iteration 35: train_loss: 0.107975, val_loss: 0.112007, train_accuracy: 0.974798, val_accuracy: 0.975362\n",
      "Iteration 40: train_loss: 0.101937, val_loss: 0.106314, train_accuracy: 0.976478, val_accuracy: 0.979710\n",
      "Iteration 45: train_loss: 0.096880, val_loss: 0.101595, train_accuracy: 0.977929, val_accuracy: 0.979710\n",
      "Iteration 50: train_loss: 0.092563, val_loss: 0.097609, train_accuracy: 0.978845, val_accuracy: 0.982609\n",
      "Iteration 55: train_loss: 0.088820, val_loss: 0.094189, train_accuracy: 0.979685, val_accuracy: 0.984058\n",
      "Iteration 60: train_loss: 0.085533, val_loss: 0.091217, train_accuracy: 0.980678, val_accuracy: 0.984058\n",
      "Iteration 65: train_loss: 0.082614, val_loss: 0.088608, train_accuracy: 0.980984, val_accuracy: 0.984058\n",
      "Iteration 70: train_loss: 0.079999, val_loss: 0.086295, train_accuracy: 0.981824, val_accuracy: 0.984058\n",
      "Iteration 75: train_loss: 0.077637, val_loss: 0.084230, train_accuracy: 0.982358, val_accuracy: 0.984058\n",
      "Iteration 80: train_loss: 0.075490, val_loss: 0.082373, train_accuracy: 0.982969, val_accuracy: 0.985507\n",
      "Iteration 85: train_loss: 0.073525, val_loss: 0.080693, train_accuracy: 0.983428, val_accuracy: 0.985507\n",
      "Iteration 90: train_loss: 0.071719, val_loss: 0.079166, train_accuracy: 0.983886, val_accuracy: 0.986957\n",
      "Iteration 95: train_loss: 0.070050, val_loss: 0.077770, train_accuracy: 0.984344, val_accuracy: 0.986957\n"
     ]
    }
   ],
   "source": [
    "sr = SoftmaxRegression(0.001, 100)\n",
    "sr.fit(X_train, y_train, X_valid, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
